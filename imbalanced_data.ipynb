{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (<ipython-input-1-fd24ebce1d12>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fd24ebce1d12>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc,\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, \n",
    "from tqdm import tqdm, notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_hiden, first_hiden = None, center_hiden = None , last_hiden = 2): \n",
    "        if first_hiden == None:\n",
    "            first_hiden = input_dim\n",
    "        if center_hiden == None:\n",
    "            center_hiden = first_hiden\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.leyer_list = nn.ModuleList()\n",
    "        layer_w = input_dim\n",
    "        hiden_dim = first_hiden\n",
    "        for i in range(n_hiden+1):\n",
    "            self.leyer_list.append(nn.Linear(layer_w, hiden_dim))\n",
    "            if i < (n_hiden/2)-1:\n",
    "                layer_w = hiden_dim\n",
    "                hiden_dim = (int)((i+1)*(2*(center_hiden-first_hiden)/n_hiden)+first_hiden)\n",
    "            else:\n",
    "                layer_w = hiden_dim\n",
    "                hiden_dim = (int)((i-n_hiden/2+1)*(2*(last_hiden-center_hiden)/n_hiden)+center_hiden)\n",
    "        self.leyer_list.append(nn.Linear(layer_w, 1))\n",
    "    def forward(self, x): \n",
    "        Relu = nn.LeakyReLU()\n",
    "        Sigm = nn.Sigmoid()\n",
    "        for layer in self.leyer_list[:-1]:\n",
    "            x = Relu(layer(x))\n",
    "        y = Sigm(self.leyer_list[-1](x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, features, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(features, targets):\n",
    "    targets = targets.to(device).view(-1,1).detach()\n",
    "    outputs = model(features.to(device)).detach()\n",
    "    loss_eval = loss_fun(outputs, targets)\n",
    "    pred = (outputs>0.5).float()\n",
    "    pred = (pred.to(device) == targets).sum()\n",
    "    return pred.float()/len(outputs), loss_eval.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_classification(run_hist):\n",
    "    \"\"\"Plot the training history of the classification model.\"\"\"\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,6), sharex=True)\n",
    "    x = np.arange(len(run_hist[\"train_loss\"]))\n",
    "    ax[0].plot(x, run_hist[\"train_loss\"],'b', marker='.', label=\"train loss\")\n",
    "    ax[0].plot(x, run_hist[\"test_loss\"],'r', marker='.', label=\"test loss\")\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(x, run_hist[\"train_acc\"],'b', marker='.', label=\"train accuracy\")\n",
    "    ax[1].plot(x, run_hist[\"test_acc\"],'r', marker='.', label=\"test accuracy\")\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(x, y, model):\n",
    "    output = model(x.to(device))\n",
    "    fpr, tpr, thresholds = roc_curve(y.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "           lw=lw, label='ROC curve (area = %0.2f)' % auc_val)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-81efb1e161ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"~/data2.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eventID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"track_particle_key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"track_position_z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"track_ghostProbability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"track_chi2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"track_phi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"track_position_phi\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_down\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdown_in_velo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mis_down\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_in_velo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticle_isDown_noVelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticle_isDown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"particle_isDown_noVelo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"particle_isDown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_file = \"~/data2.csv\"\n",
    "data = pd.read_csv(train_data_file)\n",
    "features = data.drop([\"Unnamed: 0\", \"eventID\", \"track_particle_key\", \"track_position_z\", \"track_ghostProbability\", \"track_chi2\", \"track_phi\", \"track_position_phi\"] , axis=1).astype(np.float64)\n",
    "targets = np.array([(is_down and down_in_velo) for is_down, down_in_velo in zip(data.particle_isDown_noVelo.values, data.particle_isDown.values)]).astype(np.float64)\n",
    "features = features.drop([\"particle_isDown_noVelo\", \"particle_isDown\", features.columns[0]], axis=1).values\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "X = scaler.transform(features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size = 0.2)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test).to(device)\n",
    "y_test = torch.FloatTensor(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 10000\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 1\n",
    "lr_rate = 0.05\n",
    "n_hiden = 10\n",
    "first_hiden = 25\n",
    "center_hiden = 20\n",
    "last_hiden = 5\n",
    "params = [n_hiden, first_hiden, center_hiden, last_hiden]\n",
    "training_set = Dataset(X_train, y_train)\n",
    "training_generator = utils.data.DataLoader(training_set, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, *params)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = {\n",
    "    \"train_loss\" : [],\n",
    "    \"test_loss\" : [],\n",
    "    \"train_acc\" : [],\n",
    "    \"test_acc\" : []\n",
    "}\n",
    "print(\"n_hiden: {}, first_hiden: {}, center_hiden: {} last_hiden: {}\".format(*params))\n",
    "loss_fun = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr_rate)\n",
    "model.to(device)\n",
    "n_epoch = len(stat_dict[\"train_acc\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
    "for epoch in range(n_epoch, epochs):\n",
    "    train_acc, train_loss = acc(X_train, y_train)\n",
    "    test_acc, test_loss = acc(X_test, y_test)\n",
    "    for batch_features, batch_targets in training_generator:\n",
    "        batch_features, batch_targets = batch_features.to(device).view(-1,input_dim), batch_targets.to(device).view(-1,1)\n",
    "        def closure(batch_features, batch_targets):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_features)\n",
    "            loss_val = loss_fun(pred, batch_targets)\n",
    "            loss_val.backward()\n",
    "        optimizer.step(closure(batch_features, batch_targets))\n",
    "        optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    stat_dict[\"test_loss\"].append(test_loss)\n",
    "    stat_dict[\"test_acc\"].append(test_acc)\n",
    "    stat_dict[\"train_loss\"].append(train_loss)    \n",
    "    stat_dict[\"train_acc\"].append(train_acc)\n",
    "    print(\"epoch {0}, train_loss {1:.4f}, train_accuracy = {2:.4f}, test_loss {3:.4f}, test_accuracy = {4:.4f}\".format(epoch+1, train_loss, train_acc, test_loss, test_acc))\n",
    "    if epoch!=0 and epoch%100==0:\n",
    "        plot_training_classification(stat_dict)\n",
    "        auc_val = plot_roc(X_test, y_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_training_classification(stat_dict)\n",
    "auc_val = plot_roc(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# outputs = model(torch.FloatTensor(X)).detach().numpy()\n",
    "# print(len(targets))\n",
    "# for i in range(1500):\n",
    "#     print(outputs[i], targets[i])\n",
    "# fpr, tpr, thresholds = roc_curve(targets, outputs)\n",
    "# auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc(torch.FloatTensor(X), torch.FloatTensor(targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
